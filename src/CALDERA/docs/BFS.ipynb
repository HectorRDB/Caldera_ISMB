{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the code\n",
    "\n",
    "This notebook is used to flesh out simple programming errors when writing code. It therefore focuses on the graph building part of the graph, and not on the testable part. Note that we assume in the code that the graph is connected, i.e that there is a path between any two nodes in the graph. The case of non-connected graphs is more complex but, fortunatly, it does not concern us.\n",
    "\n",
    "\n",
    "## Creating a toy dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import sys, os, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "sys.path.append(os.path.abspath(\"/Users/hector/Documents/BGWAS2/CALDERA/scripts\"))\n",
    "import Subgraphs\n",
    "reload(Subgraphs)\n",
    "from Subgraphs import *\n",
    "import ExploreBFS\n",
    "reload(ExploreBFS)\n",
    "from ExploreBFS import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(os.path.abspath(\"/Users/hector/Documents/BGWAS2/CALDERA/tests\"))\n",
    "import toyDataset\n",
    "Pop, neighbours, pattern, Pheno, edges = toyDataset.generateToyDataset()\n",
    "alpha = 0.05\n",
    "Lengths = np.ones(pattern.shape[0])\n",
    "G = structure(Pop, neighbours, pattern, Pheno, Lengths)\n",
    "n1s, n2s = G.ns()\n",
    "TH = chi2.isf(alpha / 1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the graph\n",
    "### First stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lmax = 100\n",
    "nNodes = G.lengths.shape[0]\n",
    "n1s, n2s = G.ns()\n",
    "nodes = [node for node in range(nNodes)]\n",
    "C = start(nodes, G.Pop, G.pattern, G.neighbours, G.lengths, TH, n1s, n2s)\n",
    "Env = np.array([S.Env for S in C], dtype = np.float64)\n",
    "Pvals = np.array([S.pval for S in C], dtype = np.float64)\n",
    "Not_Too_Large = np.array([S.length < 10 for S in C], dtype = np.bool_)\n",
    "# We then find the updated k0 we considering only subgraphs of size 1\n",
    "k0, TH = find_ko(Pvals, alpha)\n",
    "# We clean the list and we save testable subgraphs\n",
    "are_Testable = Pvals >= TH\n",
    "R = C[are_Testable]\n",
    "R_Pvals = Pvals[are_Testable]\n",
    "Keep = (Env >= TH) + Not_Too_Large\n",
    "C = C[Keep]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "998 out of 998 subgraphs explored at that step are currently potentially testable using the old k0\n",
      "New k0 is 1498\n",
      "0 subgraphs were pruned, including 100% because of size, 100% because of the pruning.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.3s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3910 out of 3910 subgraphs explored at that step are currently potentially testable using the old k0\n",
      "New k0 is 5290\n",
      "0 subgraphs were pruned, including 100% because of size, 100% because of the pruning.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    8.2s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1677 out of 1677 subgraphs explored at that step are currently potentially testable using the old k0\n",
      "New k0 is 6568\n",
      "0 subgraphs were pruned, including 100% because of size, 100% because of the pruning.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   57.5s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 out of 6 subgraphs explored at that step are currently potentially testable using the old k0\n",
      "New k0 is 6573\n",
      "0 subgraphs were pruned, including 100% because of size, 100% because of the pruning.\n",
      "\n",
      "Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   23.5s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n"
     ]
    }
   ],
   "source": [
    "threads = 1\n",
    "kmax = 10 ** 6\n",
    "Lmax = 500\n",
    "verbose = True\n",
    "output = \"None\"\n",
    "n = int(n1s.sum() + n2s.sum())\n",
    "while C.shape[0] > 0:\n",
    "    if k0 > kmax:\n",
    "        if verbose:\n",
    "            print('Reached kmax value')\n",
    "        break\n",
    "    # Compute all the parents\n",
    "\n",
    "    chunks = np.array_split(C, threads)\n",
    "    Childrens = Parallel(n_jobs=threads, verbose = 1)(delayed(expand)\n",
    "                       (chunk, G.Pop, G.pattern, G.neighbours, G.lengths, TH, n1s, n2s)\n",
    "                       for chunk in chunks)\n",
    "    C = np.concatenate(Childrens)\n",
    "    if C.shape[0] == 0:\n",
    "        break\n",
    "    Env = np.array([S.Env for S in C], dtype = np.float64)\n",
    "    Pvals = np.array([S.pval for S in C], dtype = np.float64)\n",
    "    Not_Too_Large = np.array([S.length < Lmax for S in C], dtype = np.bool_)\n",
    "\n",
    "    # We then find the updated k0 we considering only subgraphs of size 1\n",
    "    k0, TH = find_ko(np.concatenate([Pvals, R_Pvals]), alpha, k0)\n",
    "    # We clean the list and we save testable subgraphs\n",
    "    are_Testable = Pvals >= TH\n",
    "    R = np.concatenate([R[R_Pvals >= TH], C[are_Testable]])\n",
    "    R_Pvals = np.concatenate([R_Pvals[R_Pvals >= TH],\n",
    "                              Pvals[are_Testable]])\n",
    "    Keep = (Env >= TH) + Not_Too_Large\n",
    "    \n",
    "    if verbose:\n",
    "        messages(are_Testable, C.shape[0], Keep, Not_Too_Large, k0)\n",
    "    C = C[Keep]\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running an example where we know the truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Ground_truth as ground_truth\n",
    "Pop, neighbours, pattern, Pheno, edges = ground_truth.generateData1()\n",
    "Lengths = np.ones(pattern.shape[0])\n",
    "G = structure(Pop, neighbours, pattern, Pheno, Lengths)\n",
    "n1s, n2s = G.ns()\n",
    "n = int(n1s.sum() + n2s.sum())\n",
    "nNodes = G.lengths.shape[0]\n",
    "Patterns = G.pattern\n",
    "Neighbours = neighbours\n",
    "Lengths = G.lengths\n",
    "k0  =1\n",
    "TH = 0\n",
    "kmax = 100\n",
    "Lmax = 20\n",
    "C = start(np.array(range(nNodes)), G.Pop, Patterns, Neighbours, Lengths, TH, n1s, n2s)\n",
    "R = C\n",
    "while C.shape[0] > 0:\n",
    "    C = expand(C, G.Pop, Patterns, Neighbours, Lengths, TH, n1s, n2s)\n",
    "    if C.shape[0] == 0:\n",
    "        break\n",
    "    R = np.concatenate([R, C])\n",
    "assert(R.shape[0] == 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pop, neighbours, pattern, Pheno, edges = ground_truth.generateData2()\n",
    "Lengths = np.ones(pattern.shape[0])\n",
    "G = structure(Pop, neighbours, pattern, Pheno, Lengths)\n",
    "n1s, n2s = G.ns()\n",
    "n = int(n1s.sum() + n2s.sum())\n",
    "nNodes = G.lengths.shape[0]\n",
    "Patterns = G.pattern\n",
    "Neighbours = G.neighbours\n",
    "Lengths = G.lengths\n",
    "C = start(np.array(range(nNodes)), G.Pop, Patterns, Neighbours, Lengths, TH, n1s, n2s)\n",
    "R = C\n",
    "while C.shape[0] > 0:\n",
    "    C = expand(C, G.Pop, Patterns, Neighbours, Lengths, TH, n1s, n2s)\n",
    "    if C.shape[0] == 0:\n",
    "        break\n",
    "    R = np.concatenate([R, C])\n",
    "assert(R.shape[0] == 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pop, neighbours, pattern, Pheno, edges = ground_truth.generateData3()\n",
    "Lengths = np.ones(pattern.shape[0])\n",
    "G = structure(Pop, neighbours, pattern, Pheno, Lengths)\n",
    "n1s, n2s = G.ns()\n",
    "n = int(n1s.sum() + n2s.sum())\n",
    "nNodes = G.lengths.shape[0]\n",
    "Patterns = G.pattern\n",
    "Neighbours = G.neighbours\n",
    "Lengths = G.lengths\n",
    "C = start(np.array(range(nNodes)), G.Pop, Patterns, Neighbours, Lengths, TH, n1s, n2s)\n",
    "R = C\n",
    "while C.shape[0] > 0:\n",
    "    C = expand(C, G.Pop, Patterns, Neighbours, Lengths, TH, n1s, n2s)\n",
    "    if C.shape[0] == 0:\n",
    "        break\n",
    "    R = np.concatenate([R, C])\n",
    "assert(R.shape[0] == 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pop, neighbours, pattern, Pheno, edges = ground_truth.generateData4()\n",
    "Lengths = np.ones(pattern.shape[0])\n",
    "G = structure(Pop, neighbours, pattern, Pheno, Lengths)\n",
    "n1s, n2s = G.ns()\n",
    "n = int(n1s.sum() + n2s.sum())\n",
    "nNodes = G.lengths.shape[0]\n",
    "Patterns = G.pattern\n",
    "Neighbours = G.neighbours\n",
    "Lengths = G.lengths\n",
    "C = start(np.array(range(nNodes)), G.Pop, Patterns, Neighbours, Lengths, TH, n1s, n2s)\n",
    "R = C\n",
    "while C.shape[0] > 0:\n",
    "    C = expand(C, G.Pop, Patterns, Neighbours, Lengths, TH, n1s, n2s)\n",
    "    if C.shape[0] == 0:\n",
    "        break\n",
    "    R = np.concatenate([R, C])\n",
    "assert(R.shape[0] == 13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbours, neighbours, pattern, Pheno, edges = toyDataset.generateToyDataset(n=2, N = 10)\n",
    "Lengths = np.ones(pattern.shape[0])\n",
    "Pop = np.zeros((4,), dtype = np.int)\n",
    "G = structure(Pop, neighbours, pattern, Pheno, Lengths)\n",
    "n1s, n2s = G.ns()\n",
    "n = int(n1s.sum() + n2s.sum())\n",
    "nNodes = G.lengths.shape[0]\n",
    "Patterns = G.pattern\n",
    "Neighbours = G.neighbours\n",
    "Lengths = G.lengths\n",
    "C = start(np.array(range(nNodes)), G.Pop, Patterns, Neighbours, Lengths, TH, n1s, n2s)\n",
    "R = C\n",
    "while C.shape[0] > 0:\n",
    "    C = expand(C, G.Pop, Patterns, Neighbours, Lengths, TH, n1s, n2s)\n",
    "    if C.shape[0] == 0:\n",
    "        break\n",
    "    R = np.concatenate([R, C])\n",
    "for S in R:\n",
    "    closure = (pattern[S.neighbours,] | S.ys) == S.ys\n",
    "    closure = closure.all(axis = -1)\n",
    "    assert(not closure.any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbours, neighbours, pattern, Pheno, edges = toyDataset.generateToyDataset(n=50, N = 200)\n",
    "Lengths = np.ones(pattern.shape[0])\n",
    "Pop = np.zeros((100,), dtype = np.int)\n",
    "G = structure(Pop, neighbours, pattern, Pheno, Lengths)\n",
    "n1s, n2s = G.ns()\n",
    "n = int(n1s.sum() + n2s.sum())\n",
    "nNodes = G.lengths.shape[0]\n",
    "Patterns = G.pattern\n",
    "Neighbours = G.neighbours\n",
    "Lengths = G.lengths\n",
    "C = start(np.array(range(nNodes)), G.Pop, Patterns, Neighbours, Lengths, TH, n1s, n2s)\n",
    "R = C\n",
    "while C.shape[0] > 0:\n",
    "    C = expand(C, G.Pop, Patterns, Neighbours, Lengths, TH, n1s, n2s)\n",
    "    if C.shape[0] == 0:\n",
    "        break\n",
    "    R = np.concatenate([R, C])\n",
    "for S in R:\n",
    "    closure = (pattern[S.neighbours,] | S.ys) == S.ys\n",
    "    closure = closure.all(axis = -1)\n",
    "    assert(not closure.any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
